# ActiveVision-VLN-PaperLists
[Leaderboard of VLN](https://evalai.cloudcv.org/web/challenges/challenge-page/97/leaderboard/270)

A list of  papers in ActiveVision and VLN

## Dataset, Environment & Metrics
 - Matterport3D: Learning from RGB-D Data in Indoor Environments (***3DV*** 2017) [[pdf](https://arxiv.org/pdf/1709.06158.pdf)]
 - Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments (***CVPR*** 2018) [[pdf](https://arxiv.org/pdf/1711.07280.pdf)]
 - On Evaluation of Embodied Navigation Agents [[pdf](https://arxiv.org/pdf/1807.06757.pdf)]

## Methods for VLN
### Seq2seq - the most basic baseline
 - Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments (***CVPR*** 2018) [[pdf](https://arxiv.org/pdf/1711.07280.pdf)]
### Methods with reinforcement learning
 - Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout (***NAACL*** 2019) [[pdf](https://arxiv.org/pdf/1904.04195.pdf)]
 - Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation (***CVPR*** 2019) [[pdf](https://arxiv.org/pdf/1811.10092.pdf)]
 - Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation (***ECCV*** 2018) [[pdf](https://arxiv.org/pdf/1803.07729.pdf)]
### Methods without reinforcement learning
 - The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation (***CVPR*** 2019) [[pdf](https://arxiv.org/pdf/1903.01602.pdf)]
 - Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation (***CVPR*** 2019) [[pdf](https://arxiv.org/pdf/1903.02547.pdf)]
 - Self-Monitoring Navigation Agent via Auxiliary Progress Estimation (***ICLR*** 2019) [[pdf](https://arxiv.org/pdf/1901.03035.pdf)]
 - Speaker-Follower Models for Vision-and-Language Navigation (***NeurIPS*** 2018) [[pdf](https://arxiv.org/pdf/1806.02724.pdf)]
### Methods using imitation learning to explore the unseen environment
 - Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout (***NAACL*** 2019) [[pdf](https://arxiv.org/pdf/1904.04195.pdf)]
 - Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation (***CVPR*** 2019) [[pdf](https://arxiv.org/pdf/1811.10092.pdf)]

## Related Methods
 - Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering (***CVPR*** 2018) [[pdf](https://arxiv.org/pdf/1707.07998.pdf)]
 - Guided Feature Transformation (GFT): A Neural Language Grounding Module for Embodied Agents (***CoRL*** 2018) [[pdf](https://arxiv.org/pdf/1805.08329.pdf)]
 - Neural Modular Control for Embodied Question Answering (***CoRL*** 2018) [[pdf](https://arxiv.org/pdf/1810.11181.pdf)]

## Further Reading (Active vision methods, different problem setting, etc)
 - Adaptive Object Detection Using Adjacency and Zoom Prediction (***CVPR*** 2016) [[pdf](https://arxiv.org/pdf/1512.07711.pdf)]
 - Ecological Active Vision: Four Bioinspired Principles to Integrate Bottom-Up and Adaptive Top-Down Attention Tested With a Simple Camera-Arm Robot (***TAMD*** 2015) [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6863681)]
 - Toward predictive machine learning for active vision (***ICLR*** 2018) [[pdf](https://arxiv.org/pdf/1710.10460.pdf)]
 - Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning (***ICRA*** 2017) [[pdf](https://arxiv.org/pdf/1609.05143.pdf)]
 - Learning to Follow Directions in Street View [[pdf](https://arxiv.org/pdf/1903.00401.pdf)]
 - Habitat: A Platform for Embodied AI Research [[pdf](https://arxiv.org/pdf/1904.01201.pdf)]

